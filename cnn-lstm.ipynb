{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13567769,"sourceType":"datasetVersion","datasetId":8618782},{"sourceId":13568153,"sourceType":"datasetVersion","datasetId":8619072}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas openpyxl scikit-learn imbalanced-learn shap tensorflow\nprint(\"✅ Libraries installed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:28.938412Z","iopub.execute_input":"2025-10-31T12:50:28.938630Z","iopub.status.idle":"2025-10-31T12:50:40.574167Z","shell.execute_reply.started":"2025-10-31T12:50:28.938608Z","shell.execute_reply":"2025-10-31T12:50:40.572862Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.75.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.6.1\n✅ Libraries installed successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# The path to your CSV file\nfile_name = \"/kaggle/input/fakejobposting3/fake_job_postings.csv\" \n\n# Use pd.read_csv() instead of pd.read_excel()\ndf = pd.read_csv(file_name)\n\nprint(f\"\\n✅ Successfully loaded: {file_name}\")\nprint(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:40.576623Z","iopub.execute_input":"2025-10-31T12:50:40.576874Z","iopub.status.idle":"2025-10-31T12:50:42.543004Z","shell.execute_reply.started":"2025-10-31T12:50:40.576849Z","shell.execute_reply":"2025-10-31T12:50:42.542144Z"}},"outputs":[{"name":"stdout","text":"\n✅ Successfully loaded: /kaggle/input/fakejobposting3/fake_job_postings.csv\n   job_id                                      title            location  \\\n0       1                           Marketing Intern    US, NY, New York   \n1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n3       4          Account Executive - Washington DC  US, DC, Washington   \n4       5                        Bill Review Manager  US, FL, Fort Worth   \n\n  department salary_range                                    company_profile  \\\n0  Marketing          NaN  We're Food52, and we've created a groundbreaki...   \n1    Success          NaN  90 Seconds, the worlds Cloud Video Production ...   \n2        NaN          NaN  Valor Services provides Workforce Solutions th...   \n3      Sales          NaN  Our passion for improving quality of life thro...   \n4        NaN          NaN  SpotSource Solutions LLC is a Global Human Cap...   \n\n                                         description  \\\n0  Food52, a fast-growing, James Beard Award-winn...   \n1  Organised - Focused - Vibrant - Awesome!Do you...   \n2  Our client, located in Houston, is actively se...   \n3  THE COMPANY: ESRI – Environmental Systems Rese...   \n4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n\n                                        requirements  \\\n0  Experience with content management systems a m...   \n1  What we expect from you:Your key responsibilit...   \n2  Implement pre-commissioning and commissioning ...   \n3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n4  QUALIFICATIONS:RN license in the State of Texa...   \n\n                                            benefits  telecommuting  \\\n0                                                NaN              0   \n1  What you will get from usThrough being part of...              0   \n2                                                NaN              0   \n3  Our culture is anything but corporate—we have ...              0   \n4                              Full Benefits Offered              0   \n\n   has_company_logo  has_questions employment_type required_experience  \\\n0                 1              0           Other          Internship   \n1                 1              0       Full-time      Not Applicable   \n2                 1              0             NaN                 NaN   \n3                 1              0       Full-time    Mid-Senior level   \n4                 1              1       Full-time    Mid-Senior level   \n\n  required_education                   industry              function  \\\n0                NaN                        NaN             Marketing   \n1                NaN  Marketing and Advertising      Customer Service   \n2                NaN                        NaN                   NaN   \n3  Bachelor's Degree          Computer Software                 Sales   \n4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n\n   fraudulent  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"--- Phase 1: Feature Engineering ---\")\n\n# a. Create \"missingness\" features\ndf['is_company_profile_missing'] = df['company_profile'].isnull().astype(int)\ndf['is_requirements_missing'] = df['requirements'].isnull().astype(int)\ndf['is_benefits_missing'] = df['benefits'].isnull().astype(int)\ndf['is_salary_range_missing'] = df['salary_range'].isnull().astype(int)\n\n# b. Combine all text fields into one for analysis\ntext_columns = ['title', 'location', 'department', 'company_profile', \n                'description', 'requirements', 'benefits', 'function']\n\n# Fill NaNs with empty strings to avoid errors\nfor col in text_columns:\n    df[col] = df[col].fillna('')\n\ndf['text_combined'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n\nprint(\"✅ New features created:\")\nprint(df[['is_company_profile_missing', 'is_salary_range_missing', 'text_combined']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:42.543844Z","iopub.execute_input":"2025-10-31T12:50:42.544207Z","iopub.status.idle":"2025-10-31T12:50:42.818129Z","shell.execute_reply.started":"2025-10-31T12:50:42.544184Z","shell.execute_reply":"2025-10-31T12:50:42.817334Z"}},"outputs":[{"name":"stdout","text":"--- Phase 1: Feature Engineering ---\n✅ New features created:\n   is_company_profile_missing  is_salary_range_missing  \\\n0                           0                        1   \n1                           0                        1   \n2                           0                        1   \n3                           0                        1   \n4                           0                        1   \n\n                                       text_combined  \n0  Marketing Intern US, NY, New York Marketing We...  \n1  Customer Service - Cloud Video Production NZ, ...  \n2  Commissioning Machinery Assistant (CMA) US, IA...  \n3  Account Executive - Washington DC US, DC, Wash...  \n4  Bill Review Manager US, FL, Fort Worth  SpotSo...  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n    Cleans a raw text string:\n    1. Lowercases\n    2. Removes HTML tags\n    3. Removes punctuation and numbers\n    4. Tokenizes\n    5. Removes stopwords\n    \"\"\"\n    text = text.lower()  # Lowercase\n    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove punctuation/numbers\n    tokens = word_tokenize(text)  # Tokenize\n    # Remove stop words\n    cleaned_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n    return ' '.join(cleaned_tokens)\n\nprint(\"✅ `clean_text` function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:42.819096Z","iopub.execute_input":"2025-10-31T12:50:42.819417Z","iopub.status.idle":"2025-10-31T12:50:42.826193Z","shell.execute_reply.started":"2025-10-31T12:50:42.819386Z","shell.execute_reply":"2025-10-31T12:50:42.825286Z"}},"outputs":[{"name":"stdout","text":"✅ `clean_text` function defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:42.827174Z","iopub.execute_input":"2025-10-31T12:50:42.827419Z","iopub.status.idle":"2025-10-31T12:50:44.445183Z","shell.execute_reply.started":"2025-10-31T12:50:42.827398Z","shell.execute_reply":"2025-10-31T12:50:44.444226Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# --- Standard Libraries ---\nimport pandas as pd\nimport numpy as np\nimport re\nimport warnings\n\n# --- NLTK for Text Processing ---\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# --- Scikit-learn (sklearn) ---\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# --- TensorFlow & Keras (for\n# CNN+LSTM) ---\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Dense, concatenate, Dropout\n\n# --- Setup ---\nwarnings.filterwarnings('ignore') # Suppress warnings for cleaner output\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)\nstop_words = set(stopwords.words('english'))\n\nprint(\"✅ All libraries imported and NLTK data downloaded. Ready for CNN+LSTM.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:50:44.446176Z","iopub.execute_input":"2025-10-31T12:50:44.446654Z","iopub.status.idle":"2025-10-31T12:51:04.197898Z","shell.execute_reply.started":"2025-10-31T12:50:44.446627Z","shell.execute_reply":"2025-10-31T12:51:04.197073Z"}},"outputs":[{"name":"stderr","text":"2025-10-31 12:50:46.880529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761915047.215769      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761915047.312200      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ All libraries imported and NLTK data downloaded. Ready for CNN+LSTM.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nprint(\"Cleaning all text data... (This may take a moment)\")\ndf['text_cleaned'] = df['text_combined'].apply(clean_text)\nprint(\"✅ Text cleaning complete.\")\nprint(df[['text_combined', 'text_cleaned']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:51:04.200016Z","iopub.execute_input":"2025-10-31T12:51:04.200539Z","iopub.status.idle":"2025-10-31T12:51:31.272420Z","shell.execute_reply.started":"2025-10-31T12:51:04.200513Z","shell.execute_reply":"2025-10-31T12:51:31.271457Z"}},"outputs":[{"name":"stdout","text":"Cleaning all text data... (This may take a moment)\n✅ Text cleaning complete.\n                                       text_combined  \\\n0  Marketing Intern US, NY, New York Marketing We...   \n1  Customer Service - Cloud Video Production NZ, ...   \n2  Commissioning Machinery Assistant (CMA) US, IA...   \n3  Account Executive - Washington DC US, DC, Wash...   \n4  Bill Review Manager US, FL, Fort Worth  SpotSo...   \n\n                                        text_cleaned  \n0  marketing intern new york marketing food creat...  \n1  customer service cloud video production auckla...  \n2  commissioning machinery assistant cma wever va...  \n3  account executive washington washington sales ...  \n4  bill review manager fort worth spotsource solu...  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"--- Phase 2: Defining Features & Target ---\")\n\ntarget = 'fraudulent'\n\n# Define which columns go into which preprocessing step\nbinary_features = [\n    'telecommuting', 'has_company_logo', 'has_questions', \n    'is_company_profile_missing', 'is_requirements_missing', \n    'is_benefits_missing', 'is_salary_range_missing'\n]\ncategorical_features = ['employment_type', 'required_experience', 'required_education', 'industry']\ntext_feature = 'text_cleaned' # Our preprocessed text\n\n# Handle missing values in categorical features *before* splitting\ndf[categorical_features] = df[categorical_features].fillna('Missing')\n\n# Create X and y\nX = df[binary_features + categorical_features + [text_feature]]\ny = df[target]\n\nprint(f\"✅ X (features) and y (target) are defined.\")\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:51:31.273373Z","iopub.execute_input":"2025-10-31T12:51:31.273781Z","iopub.status.idle":"2025-10-31T12:51:31.295033Z","shell.execute_reply.started":"2025-10-31T12:51:31.273751Z","shell.execute_reply":"2025-10-31T12:51:31.293781Z"}},"outputs":[{"name":"stdout","text":"--- Phase 2: Defining Features & Target ---\n✅ X (features) and y (target) are defined.\nShape of X: (17880, 12)\nShape of y: (17880,)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.3,    # 30% of data for testing\n    random_state=42,  # Ensures reproducible results\n    stratify=y        # Keeps the same % of fake jobs in train and test\n)\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")\nprint(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\nprint(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:51:31.296025Z","iopub.execute_input":"2025-10-31T12:51:31.296352Z","iopub.status.idle":"2025-10-31T12:51:31.356375Z","shell.execute_reply.started":"2025-10-31T12:51:31.296310Z","shell.execute_reply":"2025-10-31T12:51:31.355395Z"}},"outputs":[{"name":"stdout","text":"Training set shape: (12516, 12)\nTesting set shape: (5364, 12)\nTraining target distribution:\nfraudulent\n0    0.951582\n1    0.048418\nName: proportion, dtype: float64\nTesting target distribution:\nfraudulent\n0    0.951529\n1    0.048471\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- Keras Text Preprocessing Settings ---\n# We define these constants here for our new text pipeline\nMAX_VOCAB_SIZE = 20000 # Max words to keep in vocabulary\nMAX_SEQUENCE_LENGTH = 250 # Max length of a job description\n\n# --- Keras Tabular Preprocessing ---\n# We create a preprocessor *only* for the non-text features.\n# The text feature will be handled separately by the Keras Tokenizer.\n\ntabular_preprocessor = ColumnTransformer(\n    transformers=[\n        # (name, transformer, columns_to_apply_to)\n        # Use sparse_output=False so Keras gets a dense array\n        ('categorical', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n        ('binary', 'passthrough', binary_features) # 'passthrough' leaves these columns as-is\n    ],\n    remainder='drop' # Drop any columns not specified\n)\n\nprint(\"✅ Tabular preprocessor created.\")\nprint(f\"✅ Text constants defined (MAX_VOCAB_SIZE={MAX_VOCAB_SIZE}, MAX_SEQUENCE_LENGTH={MAX_SEQUENCE_LENGTH}).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:51:31.357595Z","iopub.execute_input":"2025-10-31T12:51:31.357987Z","iopub.status.idle":"2025-10-31T12:51:31.364977Z","shell.execute_reply.started":"2025-10-31T12:51:31.357955Z","shell.execute_reply":"2025-10-31T12:51:31.364018Z"}},"outputs":[{"name":"stdout","text":"✅ Tabular preprocessor created.\n✅ Text constants defined (MAX_VOCAB_SIZE=20000, MAX_SEQUENCE_LENGTH=250).\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# === 1. Prepare Text Data (Tokenizer) ===\nprint(\"Preparing text data...\")\n# Initialize Keras Tokenizer\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n# Fit on training text\ntokenizer.fit_on_texts(X_train[text_feature])\n\n# Convert text to padded sequences\nX_train_seq = tokenizer.texts_to_sequences(X_train[text_feature])\nX_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n\nX_test_seq = tokenizer.texts_to_sequences(X_test[text_feature])\nX_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n\n# The true vocabulary size (including OOV and padding)\nvocab_size = len(tokenizer.word_index) + 1\nprint(f\"Text data prepared. Vocab size: {vocab_size}, Sequence length: {MAX_SEQUENCE_LENGTH}\")\n\n# === 2. Prepare Tabular Data (ColumnTransformer) ===\nprint(\"Preparing tabular data...\")\n# Get column names for the tabular preprocessor\ntabular_features = categorical_features + binary_features\n\n# Fit and transform the training data\nX_train_tab = tabular_preprocessor.fit_transform(X_train[tabular_features])\n# Transform the test data\nX_test_tab = tabular_preprocessor.transform(X_test[tabular_features])\n\n# Get shape of the processed tabular data\ntabular_shape = X_train_tab.shape[1]\nprint(f\"Tabular data prepared. Input shape: ({tabular_shape},)\")\n\n# === 3. Define the CNN + LSTM Model ===\nprint(\"Building the model...\")\nEMBEDDING_DIM = 100\n\n# --- Text Input Branch ---\ntext_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\nembedding_layer = Embedding(input_dim=vocab_size,\n                            output_dim=EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH)(text_input)\n\n# CNN part\nconv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\npool_layer = GlobalMaxPooling1D()(conv_layer)\n\n# LSTM part (re-uses the embedding layer as input)\nlstm_layer = LSTM(units=128, dropout=0.2)(embedding_layer)\n\n# Combine text branches\ntext_branch = concatenate([pool_layer, lstm_layer])\ntext_branch = Dropout(0.5)(text_branch)\n\n# --- Tabular Input Branch ---\ntabular_input = Input(shape=(tabular_shape,), name='tabular_input')\ntabular_branch = Dense(64, activation='relu')(tabular_input)\ntabular_branch = Dropout(0.3)(tabular_branch)\n\n# --- Merge All Branches ---\nmerged = concatenate([text_branch, tabular_branch])\nmerged_dense = Dense(128, activation='relu')(merged)\nmerged_dense = Dropout(0.5)(merged_dense)\noutput = Dense(1, activation='sigmoid', name='output')(merged_dense) # Sigmoid for binary classification\n\n# --- Create and Compile Model ---\nmodel = Model(inputs=[text_input, tabular_input], outputs=output)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\nmodel.summary()\n\n# === 4. Handle Class Imbalance (Class Weights) ===\n# We use class weights instead of SMOTE, which is much simpler for Keras\nprint(\"Calculating class weights...\")\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\nprint(f\"Class weights: {class_weight_dict}\")\n\n# === 5. Train the Model ===\nprint(\"Training the model...\")\nhistory = model.fit(\n    [X_train_pad, X_train_tab],  # Pass inputs as a list\n    y_train,\n    epochs=3,                    # Keep low for a quick test\n    batch_size=64,\n    validation_split=0.1,\n    class_weight=class_weight_dict # Pass the weights here\n)\nprint(\"✅ Model training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:51:31.366041Z","iopub.execute_input":"2025-10-31T12:51:31.366365Z","iopub.status.idle":"2025-10-31T12:55:04.156408Z","shell.execute_reply.started":"2025-10-31T12:51:31.366336Z","shell.execute_reply":"2025-10-31T12:55:04.155448Z"}},"outputs":[{"name":"stdout","text":"Preparing text data...\nText data prepared. Vocab size: 78970, Sequence length: 250\nPreparing tabular data...\nTabular data prepared. Input shape: (165,)\nBuilding the model...\n","output_type":"stream"},{"name":"stderr","text":"2025-10-31 12:51:35.463726: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │  \u001b[38;5;34m7,897,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m246\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m64,128\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m117,248\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ tabular_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m165\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m10,624\u001b[0m │ tabular_input[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m41,088\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,897,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ tabular_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,624</span> │ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,130,217\u001b[0m (31.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,130,217</span> (31.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,130,217\u001b[0m (31.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,130,217</span> (31.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Calculating class weights...\nClass weights: {0: 0.52544080604534, 1: 10.326732673267326}\nTraining the model...\nEpoch 1/3\n\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 392ms/step - accuracy: 0.8012 - loss: 0.5916 - precision: 0.1292 - recall: 0.5558 - val_accuracy: 0.8986 - val_loss: 0.2487 - val_precision: 0.2800 - val_recall: 0.9800\nEpoch 2/3\n\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 379ms/step - accuracy: 0.9246 - loss: 0.1681 - precision: 0.3821 - recall: 0.9461 - val_accuracy: 0.9457 - val_loss: 0.1377 - val_precision: 0.4196 - val_recall: 0.9400\nEpoch 3/3\n\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 379ms/step - accuracy: 0.9748 - loss: 0.0625 - precision: 0.6594 - recall: 0.9896 - val_accuracy: 0.9704 - val_loss: 0.0696 - val_precision: 0.5867 - val_recall: 0.8800\n✅ Model training complete.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"Evaluating the model on the test set...\")\n\n# Predict probabilities (Keras model outputs probabilities)\ny_pred_proba = model.predict([X_test_pad, X_test_tab])\n\n# Convert probabilities to binary classes (0 or 1) using a 0.5 threshold\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# --- Classification Report ---\n# Pay close attention to the precision and recall for \"Fake (1)\"\n# Precision (Fake): Of all jobs we flagged as fake, what % was *actually* fake?\n# Recall (Fake): Of all *actual* fake jobs, what % did we *catch*?\nprint(classification_report(y_test, y_pred, target_names=['Real (0)', 'Fake (1)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:55:04.157591Z","iopub.execute_input":"2025-10-31T12:55:04.157836Z","iopub.status.idle":"2025-10-31T12:55:19.943647Z","shell.execute_reply.started":"2025-10-31T12:55:04.157817Z","shell.execute_reply":"2025-10-31T12:55:19.942650Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model on the test set...\n\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 91ms/step\n              precision    recall  f1-score   support\n\n    Real (0)       0.99      0.98      0.98      5104\n    Fake (1)       0.65      0.88      0.75       260\n\n    accuracy                           0.97      5364\n   macro avg       0.82      0.93      0.87      5364\nweighted avg       0.98      0.97      0.97      5364\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}