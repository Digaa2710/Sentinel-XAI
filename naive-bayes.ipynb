{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13567769,"sourceType":"datasetVersion","datasetId":8618782}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas openpyxl scikit-learn imbalanced-learn shap\nprint(\"✅ Libraries installed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:28.902038Z","iopub.execute_input":"2025-10-31T12:23:28.902453Z","iopub.status.idle":"2025-10-31T12:23:33.046785Z","shell.execute_reply.started":"2025-10-31T12:23:28.902426Z","shell.execute_reply":"2025-10-31T12:23:33.045384Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n✅ Libraries installed successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\n\n# The path to your CSV file\nfile_name = \"/kaggle/input/fakejobposting/fake_job_postings.csv\" \n\n# Use pd.read_csv() instead of pd.read_excel()\ndf = pd.read_csv(file_name)\n\nprint(f\"\\n✅ Successfully loaded: {file_name}\")\nprint(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:33.048910Z","iopub.execute_input":"2025-10-31T12:23:33.049196Z","iopub.status.idle":"2025-10-31T12:23:33.862027Z","shell.execute_reply.started":"2025-10-31T12:23:33.049170Z","shell.execute_reply":"2025-10-31T12:23:33.861018Z"}},"outputs":[{"name":"stdout","text":"\n✅ Successfully loaded: /kaggle/input/fakejobposting/fake_job_postings.csv\n   job_id                                      title            location  \\\n0       1                           Marketing Intern    US, NY, New York   \n1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n3       4          Account Executive - Washington DC  US, DC, Washington   \n4       5                        Bill Review Manager  US, FL, Fort Worth   \n\n  department salary_range                                    company_profile  \\\n0  Marketing          NaN  We're Food52, and we've created a groundbreaki...   \n1    Success          NaN  90 Seconds, the worlds Cloud Video Production ...   \n2        NaN          NaN  Valor Services provides Workforce Solutions th...   \n3      Sales          NaN  Our passion for improving quality of life thro...   \n4        NaN          NaN  SpotSource Solutions LLC is a Global Human Cap...   \n\n                                         description  \\\n0  Food52, a fast-growing, James Beard Award-winn...   \n1  Organised - Focused - Vibrant - Awesome!Do you...   \n2  Our client, located in Houston, is actively se...   \n3  THE COMPANY: ESRI – Environmental Systems Rese...   \n4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n\n                                        requirements  \\\n0  Experience with content management systems a m...   \n1  What we expect from you:Your key responsibilit...   \n2  Implement pre-commissioning and commissioning ...   \n3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n4  QUALIFICATIONS:RN license in the State of Texa...   \n\n                                            benefits  telecommuting  \\\n0                                                NaN              0   \n1  What you will get from usThrough being part of...              0   \n2                                                NaN              0   \n3  Our culture is anything but corporate—we have ...              0   \n4                              Full Benefits Offered              0   \n\n   has_company_logo  has_questions employment_type required_experience  \\\n0                 1              0           Other          Internship   \n1                 1              0       Full-time      Not Applicable   \n2                 1              0             NaN                 NaN   \n3                 1              0       Full-time    Mid-Senior level   \n4                 1              1       Full-time    Mid-Senior level   \n\n  required_education                   industry              function  \\\n0                NaN                        NaN             Marketing   \n1                NaN  Marketing and Advertising      Customer Service   \n2                NaN                        NaN                   NaN   \n3  Bachelor's Degree          Computer Software                 Sales   \n4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n\n   fraudulent  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"--- Phase 1: Feature Engineering ---\")\n\n# a. Create \"missingness\" features\ndf['is_company_profile_missing'] = df['company_profile'].isnull().astype(int)\ndf['is_requirements_missing'] = df['requirements'].isnull().astype(int)\ndf['is_benefits_missing'] = df['benefits'].isnull().astype(int)\ndf['is_salary_range_missing'] = df['salary_range'].isnull().astype(int)\n\n# b. Combine all text fields into one for analysis\ntext_columns = ['title', 'location', 'department', 'company_profile', \n                'description', 'requirements', 'benefits', 'function']\n\n# Fill NaNs with empty strings to avoid errors\nfor col in text_columns:\n    df[col] = df[col].fillna('')\n\ndf['text_combined'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n\nprint(\"✅ New features created:\")\nprint(df[['is_company_profile_missing', 'is_salary_range_missing', 'text_combined']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:33.862927Z","iopub.execute_input":"2025-10-31T12:23:33.863266Z","iopub.status.idle":"2025-10-31T12:23:34.039418Z","shell.execute_reply.started":"2025-10-31T12:23:33.863231Z","shell.execute_reply":"2025-10-31T12:23:34.038320Z"}},"outputs":[{"name":"stdout","text":"--- Phase 1: Feature Engineering ---\n✅ New features created:\n   is_company_profile_missing  is_salary_range_missing  \\\n0                           0                        1   \n1                           0                        1   \n2                           0                        1   \n3                           0                        1   \n4                           0                        1   \n\n                                       text_combined  \n0  Marketing Intern US, NY, New York Marketing We...  \n1  Customer Service - Cloud Video Production NZ, ...  \n2  Commissioning Machinery Assistant (CMA) US, IA...  \n3  Account Executive - Washington DC US, DC, Wash...  \n4  Bill Review Manager US, FL, Fort Worth  SpotSo...  \n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n    Cleans a raw text string:\n    1. Lowercases\n    2. Removes HTML tags\n    3. Removes punctuation and numbers\n    4. Tokenizes\n    5. Removes stopwords\n    \"\"\"\n    text = text.lower()  # Lowercase\n    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove punctuation/numbers\n    tokens = word_tokenize(text)  # Tokenize\n    # Remove stop words\n    cleaned_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n    return ' '.join(cleaned_tokens)\n\nprint(\"✅ `clean_text` function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:34.041651Z","iopub.execute_input":"2025-10-31T12:23:34.041893Z","iopub.status.idle":"2025-10-31T12:23:34.048351Z","shell.execute_reply.started":"2025-10-31T12:23:34.041872Z","shell.execute_reply":"2025-10-31T12:23:34.047455Z"}},"outputs":[{"name":"stdout","text":"✅ `clean_text` function defined.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:34.049195Z","iopub.execute_input":"2025-10-31T12:23:34.049421Z","iopub.status.idle":"2025-10-31T12:23:34.066872Z","shell.execute_reply.started":"2025-10-31T12:23:34.049401Z","shell.execute_reply":"2025-10-31T12:23:34.065964Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# --- Standard Libraries ---\nimport pandas as pd\nimport numpy as np\nimport re\nimport warnings\n\n# --- NLTK for Text Processing ---\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# --- Scikit-learn (sklearn) for Modeling ---\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.metrics import classification_report\nfrom scipy import sparse\n\n# --- Imbalanced-learn (optional) for SMOTE ---\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom imblearn.over_sampling import SMOTE\n\n# --- Setup ---\nwarnings.filterwarnings('ignore') # Suppress warnings for cleaner output\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)\nstop_words = set(stopwords.words('english'))\n\nprint(\"✅ All libraries imported and NLTK data downloaded. Ready for Naive Bayes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:34.067859Z","iopub.execute_input":"2025-10-31T12:23:34.068202Z","iopub.status.idle":"2025-10-31T12:23:34.081946Z","shell.execute_reply.started":"2025-10-31T12:23:34.068173Z","shell.execute_reply":"2025-10-31T12:23:34.081280Z"}},"outputs":[{"name":"stdout","text":"✅ All libraries imported and NLTK data downloaded. Ready for Naive Bayes.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"\nprint(\"Cleaning all text data... (This may take a moment)\")\ndf['text_cleaned'] = df['text_combined'].apply(clean_text)\nprint(\"✅ Text cleaning complete.\")\nprint(df[['text_combined', 'text_cleaned']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:23:34.083384Z","iopub.execute_input":"2025-10-31T12:23:34.084023Z","iopub.status.idle":"2025-10-31T12:24:00.764165Z","shell.execute_reply.started":"2025-10-31T12:23:34.084001Z","shell.execute_reply":"2025-10-31T12:24:00.763250Z"}},"outputs":[{"name":"stdout","text":"Cleaning all text data... (This may take a moment)\n✅ Text cleaning complete.\n                                       text_combined  \\\n0  Marketing Intern US, NY, New York Marketing We...   \n1  Customer Service - Cloud Video Production NZ, ...   \n2  Commissioning Machinery Assistant (CMA) US, IA...   \n3  Account Executive - Washington DC US, DC, Wash...   \n4  Bill Review Manager US, FL, Fort Worth  SpotSo...   \n\n                                        text_cleaned  \n0  marketing intern new york marketing food creat...  \n1  customer service cloud video production auckla...  \n2  commissioning machinery assistant cma wever va...  \n3  account executive washington washington sales ...  \n4  bill review manager fort worth spotsource solu...  \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(\"--- Phase 2: Defining Features & Target ---\")\n\ntarget = 'fraudulent'\n\n# Define which columns go into which preprocessing step\nbinary_features = [\n    'telecommuting', 'has_company_logo', 'has_questions', \n    'is_company_profile_missing', 'is_requirements_missing', \n    'is_benefits_missing', 'is_salary_range_missing'\n]\ncategorical_features = ['employment_type', 'required_experience', 'required_education', 'industry']\ntext_feature = 'text_cleaned' # Our preprocessed text\n\n# Handle missing values in categorical features *before* splitting\ndf[categorical_features] = df[categorical_features].fillna('Missing')\n\n# Create X and y\nX = df[binary_features + categorical_features + [text_feature]]\ny = df[target]\n\nprint(f\"✅ X (features) and y (target) are defined.\")\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:24:00.764873Z","iopub.execute_input":"2025-10-31T12:24:00.765166Z","iopub.status.idle":"2025-10-31T12:24:00.797578Z","shell.execute_reply.started":"2025-10-31T12:24:00.765142Z","shell.execute_reply":"2025-10-31T12:24:00.796760Z"}},"outputs":[{"name":"stdout","text":"--- Phase 2: Defining Features & Target ---\n✅ X (features) and y (target) are defined.\nShape of X: (17880, 12)\nShape of y: (17880,)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.3,    # 30% of data for testing\n    random_state=42,  # Ensures reproducible results\n    stratify=y        # Keeps the same % of fake jobs in train and test\n)\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")\nprint(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\nprint(f\"Testing target distribution:\\n{y_test.value_counts(normalize=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:24:00.798434Z","iopub.execute_input":"2025-10-31T12:24:00.798661Z","iopub.status.idle":"2025-10-31T12:24:00.827042Z","shell.execute_reply.started":"2025-10-31T12:24:00.798642Z","shell.execute_reply":"2025-10-31T12:24:00.826017Z"}},"outputs":[{"name":"stdout","text":"Training set shape: (12516, 12)\nTesting set shape: (5364, 12)\nTraining target distribution:\nfraudulent\n0    0.951582\n1    0.048418\nName: proportion, dtype: float64\nTesting target distribution:\nfraudulent\n0    0.951529\n1    0.048471\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# This pipeline applies different transforms to different columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        # (name, transformer, columns_to_apply_to)\n        ('text', TfidfVectorizer(max_features=5000, stop_words='english'), text_feature),\n        ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n        ('binary', 'passthrough', binary_features) # 'passthrough' leaves these columns as-is\n    ],\n    remainder='drop' # Drop any columns not specified\n)\n\nprint(\"✅ Preprocessing ColumnTransformer created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:24:00.829765Z","iopub.execute_input":"2025-10-31T12:24:00.830062Z","iopub.status.idle":"2025-10-31T12:24:00.836151Z","shell.execute_reply.started":"2025-10-31T12:24:00.830039Z","shell.execute_reply":"2025-10-31T12:24:00.835323Z"}},"outputs":[{"name":"stdout","text":"✅ Preprocessing ColumnTransformer created.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\n# Choose the model (MultinomialNB for counts/TF-IDF, BernoulliNB for binary features)\nnb_model = MultinomialNB()\n\n# Updated pipeline\nmodel_pipeline = ImbPipeline(steps=[\n    ('preprocessor', preprocessor),  # Your ColumnTransformer from before\n    ('smote', SMOTE(random_state=42)),  # Optional: balances classes\n    ('model', nb_model)\n])\n\n# --- Train the Model ---\nprint(\"Training the Naive Bayes model... (This may take a few minutes)\")\nmodel_pipeline.fit(X_train, y_train)\nprint(\"✅ Model training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:24:00.837160Z","iopub.execute_input":"2025-10-31T12:24:00.837456Z","iopub.status.idle":"2025-10-31T12:24:04.128809Z","shell.execute_reply.started":"2025-10-31T12:24:00.837429Z","shell.execute_reply":"2025-10-31T12:24:04.127902Z"}},"outputs":[{"name":"stdout","text":"Training the Naive Bayes model... (This may take a few minutes)\n✅ Model training complete.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"y_pred = model_pipeline.predict(X_test)\n\n# Pay close attention to the precision and recall for \"Fake (1)\"\n# Precision (Fake): Of all jobs we flagged as fake, what % was *actually* fake?\n# Recall (Fake): Of all *actual* fake jobs, what % did we *catch*?\nprint(classification_report(y_test, y_pred, target_names=['Real (0)', 'Fake (1)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:24:04.129880Z","iopub.execute_input":"2025-10-31T12:24:04.130274Z","iopub.status.idle":"2025-10-31T12:24:05.320869Z","shell.execute_reply.started":"2025-10-31T12:24:04.130241Z","shell.execute_reply":"2025-10-31T12:24:05.319769Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    Real (0)       0.99      0.92      0.96      5104\n    Fake (1)       0.37      0.90      0.52       260\n\n    accuracy                           0.92      5364\n   macro avg       0.68      0.91      0.74      5364\nweighted avg       0.96      0.92      0.93      5364\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}